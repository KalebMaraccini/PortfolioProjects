{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6313e46",
   "metadata": {},
   "source": [
    "Intent here is to use the kaggle intro courses, begining with the titanic surviaval prediction data, to build a machine learning guide. the structure will be layered; starting with a simple broad overview, followed by sections going into greater depth in one area after another, followed by more going subsequently deeper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1ce4b",
   "metadata": {},
   "source": [
    "# Ch. 1 Fist Pass\n",
    "Here we will go through a building a simple machine learning model for the Kaggle Titanic learning competetion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3dcea",
   "metadata": {},
   "source": [
    "### set up the eviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cbb5b7",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b26264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\marac\\Downloads\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\marac\\Downloads\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79963f3e",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "We will skip this in our first pass, but cover good habits in a later chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0044b0a",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Clean the data and prepare it for machine learning algorithms. Again, in this chapter we will take this more or less for granted but cover strategies in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for preprocessing to ensure consistency\n",
    "full_data = pd.concat([train.drop('Survived',axis=1),test])\n",
    "\n",
    "# extract and group titles\n",
    "## this turns a unique identifier column into something we can test for correlation with \n",
    "## survival. many titles appear to infrquently to garner their own category, group into \"Rare\"\n",
    "full_data['Title'] = full_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "title_mapping = {\n",
    "    \"Mr\": \"Mr\",\n",
    "    \"Miss\": \"Miss\",\n",
    "    \"Mrs\": \"Mrs\",\n",
    "    \"Master\": \"Master\",\n",
    "    \"Dr\": \"Rare\",\n",
    "    \"Rev\": \"Rare\",\n",
    "    \"Col\": \"Rare\",\n",
    "    \"Major\": \"Rare\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Don\": \"Rare\",\n",
    "    \"Lady\": \"Rare\",\n",
    "    \"Countess\": \"Rare\",\n",
    "    \"Jonkheer\": \"Rare\",\n",
    "    \"Sir\": \"Rare\",\n",
    "    \"Capt\": \"Rare\",\n",
    "    \"Ms\": \"Miss\",\n",
    "    \"Dona\": \"Rare\"\n",
    "}\n",
    "full_data['Title'] = full_data['Title'].map(title_mapping)\n",
    "\n",
    "# Create family size feature\n",
    "## siblings on shipt + parents on ship + self\n",
    "full_data['FamilySize'] = full_data['SibSp'] + full_data['Parch'] + 1\n",
    "\n",
    "# Create is_alone feature\n",
    "full_data['IsAlone'] = (full_data['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Create age bands\n",
    "## bining the conttinuous variable age will let us capture non-linear effects and reduce noise\n",
    "full_data['AgeBand'] = pd.cut(full_data['Age'], 5)\n",
    "\n",
    "# Get the length of cabin string (proxy for wealth/status)\n",
    "full_data['CabinBool'] = (full_data['Cabin'].notnull()).astype(int)\n",
    "\n",
    "# Create fare per person\n",
    "full_data['FarePerPerson'] = full_data['Fare'] / (full_data['FamilySize'])\n",
    "\n",
    "full_data['age_class'] =full_data['Age'] *full_data['Pclass']\n",
    "\n",
    "# Split back to train/test\n",
    "train_processed = full_data.iloc[:len(train)].copy()\n",
    "test_processed = full_data.iloc[len(train):].copy()\n",
    "\n",
    "# Recombine with survival data for training\n",
    "train_processed.loc[:, 'Survived'] = train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dac559",
   "metadata": {},
   "source": [
    "### define models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to use\n",
    "numerical_features = ['Age', 'Fare', 'FamilySize', 'FarePerPerson','age_class']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'CabinBool', 'IsAlone','AgeBand']\n",
    "\n",
    "# Define preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define models to try\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Model Evaluation\n",
    "X = train_processed[numerical_features + categorical_features]\n",
    "y = train_processed['Survived']\n",
    "\n",
    "# Split data for model evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e4211",
   "metadata": {},
   "source": [
    "### train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03407d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Create and fit the full pipeline\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on training data\n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = full_pipeline.predict(X_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(full_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcaa98d",
   "metadata": {},
   "source": [
    "And that's it. Very, overly, simply put we have successfully trained a model to predict survival."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
